<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ChatGPT-的工作原理-深入探究</title>
    <link rel="stylesheet" href="https://c1312.netlify.app/typo.css">
    <link rel="stylesheet" href="https://c1312.netlify.app/style.css">
</head>
<body>
<header>
  <h1>ChatGPT-的工作原理-深入探究</h1>
</header>
<main class="typo"><div><div><div><div><main><div><div><article><div><div>

<p>这篇文章主要探讨了ChatGPT的工作原理。ChatGPT是基于OpenAI开发的GPT-4架构的大型语言模型。首先，文章介绍了GPT的基本概念，即生成<strong>预测性网络模型</strong>。GPT模型利用大量的文本数据进行训练，从而学会在各种情境中生成连贯的文本。</p>
<p>接着，文章详细阐述了训练过程，分为<strong>预训练</strong>和<strong>微调</strong>两个阶段。在预训练阶段，模型学习理解文本数据，包括词汇、语法、事实等；在微调阶段，模型使用具有限制性任务的数据集来调整，以获得更准确的输出。作者还提到了训练数据的来源，强调了在大量网络文本数据中获取知识的重要性。</p>
<p>在解释输出生成时，文章提到了一个关键技术：<em>集束搜索（Beam Search）</em>。这是一种启发式搜索策略，用于选择最优文本序列。此外，作者强调了解决生成内容问题的策略，包括设置过滤器和调整温度参数。</p>
<p>最后，文章讨论了ChatGPT的局限性，例如处理输入数据时可能会产生偏见，或无法回答一些问题。尽管如此，作者指出ChatGPT是一个强大的工具，能够在各种任务中提供有价值的帮助。</p>
<hr/>
<p>像ChatGPT这样的大型语言模型实际上是如何工作的？嗯，它们既非常简单又极其复杂。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/b35e79b1.webp"/></p>
<p>你可以将模型视为根据某些输入计算输出概率的工具。在语言模型中，这意味着给定一系列单词，它们会计算出序列中下一个单词的概率，就像高级自动完成一样。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/50963837.webp"/></p>
<p>要理解这些概率的来源，我们需要谈论一些叫做<strong>神经网络</strong>的东西。这是一个类似网络的结构，数字被输入到一侧，概率被输出到另一侧。它们比你想象的要简单。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/765f8dc3.webp"/></p>
<p>想象一下，我们想要训练一台计算机来解决在<code>3x3</code>像素显示器上识别符号的简单问题。我们需要像这样的神经网络：</p>
<ul>
<li>一个输入层</li>
<li>两个隐藏层</li>
<li>一个输出层。</li>
</ul>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/71123a57.webp"/></p>
<p>我们的输入层由9个称为神经元的节点组成，每个像素一个。每个神经元将保存从1（白色）到-1（黑色）的数字。我们的输出层由4个神经元组成，每个神经元代表可能的符号之一。它们的值最终将是0到1之间的概率。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/05e1dc8a.webp"/></p>
<p>在这些之间，我们有一些神经元的排列，称为**“隐藏”层**。对于我们简单的用例，我们只需要两个。每个神经元都通过一个权重与相邻层中的神经元相连，该权重的值可以在<code>-</code>1和<code>1</code>之间。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/e08b466f.webp"/></p>
<p>当一个值从输入神经元传递到下一层时，它会乘以权重。然后，该神经元简单地将其接收到的所有值相加，将该值压缩在-1和1之间，并将其传递给下一层中的每个神经元。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/75e9e4e9.webp"/></p>
<p>最后一个隐藏层中的神经元执行相同的操作，但将值压缩在0和1之间，并将其传递到输出层。输出层中的每个神经元都保存一个概率，最高的数字是最可能的结果。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/de1b13eb.webp"/></p>
<p>当我们训练这个网络时，我们向它提供一个我们知道答案的图像，并计算答案与网络计算的概率之间的差异。然后我们调整权重以接近预期结果。但是我们如何知道如何调整权重呢？</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/db837c0b.webp"/></p>
<p>我们使用称为<strong>梯度下降</strong>和<strong>反向传播</strong>的巧妙数学技术来确定每个权重的哪个值会给我们最低的误差。我们不断重复这个过程，直到我们对模型的准确性感到满意。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/cc39e624.webp"/></p>
<p>这被称为<strong>前馈神经网络</strong> - 但这种简单的结构不足以解决自然语言处理的问题。相反，LLM倾向于使用一种称为<code>Transformer</code>的结构，它具有一些关键概念，可以释放出很多潜力。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/b305cde8.webp"/></p>
<p>首先，让我们谈谈单词。我们可以将单词分解为 <code>token</code> ，这些 <code>token</code> 可以是单词、子单词、字符或符号，而不是将每个单词作为输入。请注意，它们甚至包括空格。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/1b62a5eb.webp"/></p>
<p>就像我们的模型中将像素值表示为0到1之间的数字一样，这些<code>token</code>也需要表示为数字。我们可以为每个标记分配一个唯一的数字并称之为一天，但还有另一种表示它们的方式，可以添加更多上下文。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/e6dc91aa.webp"/></p>
<p>我们可以将每个 token 存储在一个多维向量中，指示它与其他标记的关系。为简单起见，想象一下在二维平面上绘制单词位置。我们希望具有相似含义的单词彼此靠近。这被称为 <strong>embedding 嵌入</strong>。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/f331b128.webp"/></p>
<p>embedding 有助于创建相似单词之间的关系，但它们也捕捉类比。例如，单词“dog”和“puppy”之间的距离应该与“cat”和“kitten”之间的距离相同。我们还可以为整个句子创建 embedding 。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/ddae0c1b.webp"/></p>
<p><code>transformer</code>  的第一部分是将我们的输入单词编码为这些 <code>embedding</code>。然后将这些嵌入馈送到下一个过程，称为 <strong>attention</strong> ，它为 <strong>embedding</strong> 添加了更多的上下文。<strong>attention</strong> 在自然语言处理中非常重要。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/a002f821.webp"/></p>
<p><strong>Embedding</strong> 难以捕捉具有多重含义的单词。考虑 <code>bank</code> 这个词的两个含义。人类根据句子的上下文推断出正确的含义。<code>Money</code> 和 <code>River</code> 在每个句子中都是与 <code>bank</code>相关的重要上下文。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/c1799278.webp"/></p>
<p><strong>attention</strong> 的过程会回顾整个句子，寻找提供词汇背景的单词。然后重新调整 embedding  权重，使得单词“river”或“money”在语义上更接近于“word bank”。</p>
<p><img src="https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/12d29427.webp"/></p>
<p>这个 <code>attention</code>  过程会多次发生，以捕捉句子在多个维度上的上下文。在所有这些过程之后，上下文 embedding 最终被传递到神经网络中，就像我们之前提到的简单神经网络一样，产生概率。</p>
<p>这是一个大大简化了的LLM（像ChatGPT这样的语言模型）工作原理的版本。为了简洁起见，本文省略或略过了很多内容。</p>
<p></p>

</div></div></article></div></div></main></div></div></div></div>
      <hr/>
      <!-- clipping information -->
      </main>
<footer>
        <label>原网址: <a href="https://juejin.cn/post/7234146188508168252" target="_blank" referrerpolicy="no-referrer" rel="noopener noreferrer">访问</a></label><br/>
        <label>创建时间: 2023-05-18 14:01:05</label><br/>
        <br/>
        
      </footer>
</body>
</html>