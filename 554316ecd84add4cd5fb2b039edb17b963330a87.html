<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>直截了当-展示给我提示符-译</title>
    <link rel="stylesheet" href="https://c1312.netlify.app/typo.css">
    <link rel="stylesheet" href="https://c1312.netlify.app/style.css">
</head>
<body>
<header>
  <h1>直截了当-展示给我提示符-译</h1>
</header>
<main class="typo"><div><p><a href="https://hamel.dev/index.html#category=llms">大语言模型</a><p><a href="https://hamel.dev/index.html#category=ml">机器学习</a><p>通过截获 API 调用，迅速掌握难以解读的大语言模型框架。<p>作者：Hamel Husain<p>发布日期：2024 年 2 月 14 日<h2><a href=#%E8%83%8C%E6%99%AF><span></span></a>背景</h2><p>众多库致力于通过<strong>自动重构或创建提示符</strong>来优化大语言模型的输出。这些建库宣称能够使大语言模型的输出更加：<ul><li>安全 <a href=https://github.com/guardrails-ai/guardrails>(例如：安全护栏)</a><li>可预测 <a href=https://github.com/guidance-ai/guidance>(例如：智能指导)</a><li>结构化 <a href=https://github.com/jxnl/instructor>(例如：指令生成器)</a><li>鲁棒 <a href=https://www.langchain.com/>(例如：语言链)</a><li>… 或者针对特定指标进行优化 <a href=https://github.com/stanfordnlp/dspy>(例如：DSPy)</a>。</ul><p>这些工具中_某些_共同的思路是鼓励用户不直接与提示符交互。<blockquote><p><a href=https://github.com/stanfordnlp/dspy>DSPy</a>：“这开启了一种新范式，语言模型及其提示符退居幕后….重新编译程序时，DSPy 会生成新的有效提示符”</blockquote><blockquote><p><a href=https://github.com/guidance-ai/guidance>智能指导</a> “智能指导代表了一种高效控制的编程范式，相比传统提示更为高效…”</blockquote><p>即便有些工具不反对使用提示符，我发现要获取这些工具最终发送给语言模型的提示符很有挑战。**这些工具发送给大语言模型的提示是对其操作的自然语言描述，是最快了解它们工作方式的途径。**此外，一些工具使用<a href="https://github.com/stanfordnlp/dspy?tab=readme-ov-file#4-two-powerful-concepts-signatures--teleprompters">复杂术语</a>来描述其内部机制，这使得理解它们的操作更加困难。<p>基于我接下来将解释的原因，我认为大多数人采用以下思维方式会更好：<p><img src=assets/1709515433-d006da6d0d7bde8e991a4454c035a606.jpeg><p>在本篇博客中，我会教你如何**不需深究文档或源代码，就能截获任何工具的 API 调用及其提示符。**我将通过使用 <a href=https://mitmproxy.org/>mitmproxy</a> 的示例，展示如何设置和操作，以便捷地理解我之前提到的工具及其大语言模型的工作原理。<h2><a href=#%E5%8A%A8%E6%9C%BA%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%87%8F%E5%B0%91%E9%9D%9E%E5%BF%85%E8%A6%81%E7%9A%84%E5%A4%8D%E6%9D%82%E6%80%A7><span></span></a>动机：尽可能减少非必要的复杂性<a href=https://hamel.dev/blog/posts/prompt/#motivation-minimize-accidental-complexity></a></h2><p>在决定采用某种抽象之前，考虑到非必要复杂性的风险是非常重要的。这一点对于大语言模型（LLM）的应用来说尤其关键，因为与编程的抽象相比，LLM 的抽象往往使用户不得不回到编码上来，而不是用自然语言与 AI 进行交流，这与 LLM 的初衷相悖：<p>尽管这听起来有些俏皮，但在评估工具时牢记这一点是非常有价值的。工具提供的自动化主要分为两类：<ul><li><strong>代码与 LLM 的结合应用：</strong> 通常情况下，通过编码来实现这类自动化是最佳选择，因为任务的执行需要运行代码。例如路由、函数执行、重试、串联等。<li><strong>重构和编写提示：</strong> 表达意图通常最适合用自然语言，但也有例外。比如，用代码而不是自然语言来定义一个函数或架构会更加方便。</ul><p>很多框架都提供了这两种自动化方式。但是，过分依赖第二种方式可能会带来不利影响。通过观察提示，你可以作出以下决定：<ol><li>这个框架是否真的必要？<li>我是否只需复制最终的提示（一个字符串），然后抛弃这个框架？<li>我们能否编写出更好的提示（更简洁，更符合意图等）？<li>这是否是最佳方案（API 调用的数量是否合适）？</ol><p>根据我的经验，观察提示和 API 调用对于做出明智的选择至关重要。<h2><a href=#%E6%8B%A6%E6%88%AA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-api-%E8%B0%83%E7%94%A8><span></span></a>拦截大语言模型 API 调用<a href=https://hamel.dev/blog/posts/prompt/#intercepting-llm-api-calls></a></h2><p>有很多方法可以拦截大语言模型 API 调用，比如修改源代码或寻找对用户友好的设置选项。我发现这些方法往往非常耗时，特别是当源代码和文档的质量参差不齐的时候。毕竟，我只是想看到 API 调用，而不需要深入了解代码是如何工作的！<p>然而，找到一个既简单又高效的方法并不容易。一个不依赖特定框架的方法是设置一个代理，用来记录你发出的 API 请求。使用<a href=https://mitmproxy.org/>mitmproxy</a>，一个免费的开源 HTTPS 代理，可以轻松实现这一点。<h3><a href=#%E9%85%8D%E7%BD%AE-mitmproxy><span></span></a>配置 mitmproxy<a href=https://hamel.dev/blog/posts/prompt/#setting-up-mitmproxy></a></h3><p>以下是一种特别适合初学者的设置 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">mitmproxy</code> 的方式，简单易懂，助你快速上手：<ol><li><p>根据 <a href=https://mitmproxy.org/>mitmproxy 官网</a> 上的指南完成安装。<li><p>在终端中运行 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">mitmweb</code> 命令启动交互式用户界面。请留意日志中显示的用户界面 URL，通常格式为：<code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">Web server listening at http://127.0.0.1:8081/</code>。<li><p>下一步，您需要设置您的设备（比如笔记本电脑），使所有网络流量都通过 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">mitproxy</code> 进行路由，该服务监听在 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">http://localhost:8080</code>。根据官方文档建议：<blockquote><p>我们推荐您上网搜索如何为您的操作系统配置 HTTP 代理。有些操作系统提供全局设置选项，有些浏览器则有自己的配置方式，还有一些应用可能需要设置环境变量等。</blockquote><p>以我的经验为例，我通过谷歌搜索“为 macOS 设置代理”，找到了如下操作方法：<blockquote><p>选择 Apple 菜单 &gt; 系统设置，点击侧栏中的网络，选择右侧的一个网络服务，点击“详情”，然后选择“代理”。</blockquote></ol><p>通过上述步骤，即使是初学者也能轻松设置并开始使用 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">mitmproxy</code>。这种方法不仅简化了安装和配置过程，还为用户提供了一种直观的方式来监控和管理网络流量。<p>我然后在界面的特定位置添加了<code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">localhost</code> 和 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">8080</code>：<p><img src=assets/1709515433-238a58667b928e13ff0830dcfa8e8ceb.png><ol><li><p>然后，访问 <a href=http://mitm.it/>http://mitm.it</a>，网站将指导你如何安装用于拦截 HTTPS 请求的 mitmproxy 证书授权机构（CA）。如果愿意，你也可以手动完成这一步骤，操作指南请参见<a href=https://docs.mitmproxy.org/stable/concepts-certificates/#quick-setup>此处</a>。请记住 CA 文件的存储位置，我们稍后会用到它。<li><p>通过访问如 <a href=https://mitmproxy.org/>https://mitmproxy.org/</a> 等网站，你可以测试一切是否正常工作。你应该能在 mtimweb 用户界面中看到相应的输出，对我来说，它的位置是 <a href=http://127.0.0.1:8081/>http://127.0.0.1:8081/</a>（查看终端日志以获取 URL）。<li><p>完成以上设置后，你可以关闭之前在网络上启用的代理。我是通过切换之前展示的屏幕截图中的代理按钮在我的 Mac 上做到这一点的。这样做是为了限制代理仅适用于 Python 程序，以避免不必要的干扰。</ol><p><strong>提示</strong>：通常，与网络相关的软件会允许你通过设置环境变量来代理外出请求。这就是我们将采取的方法，专门将代理应用于特定的 Python 程序。不过，我还是鼓励你在熟悉操作后，试试看能否在其他类型的程序中找到新的发现！<h3><a href=#python-%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F><span></span></a>Python 的环境变量</h3><p>为了让 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">requests</code> 和 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">httpx</code> 库能够通过代理转发流量并为 HTTPS 流量引用 CA 文件，我们需要设置以下环境变量：<p><strong>重要提示</strong>：请确保在执行本博客帖子中任何代码片段之前，先设置好这些环境变量。<p>你可以通过执行以下简短的代码来进行基础测试：<p>这在用户界面中的显示效果如下：<p><img src=assets/1709515433-5ac10ad20f070cf0feeec26809d3da9e.png><h2><a href=#%E7%A4%BA%E4%BE%8B><span></span></a>示例</h2><p>接下来是最有趣的部分，我们将通过运行一些大语言模型库的示例，并截获它们的 API 调用！<h3><a href=#%E5%AE%89%E5%85%A8%E9%98%B2%E6%8A%A4><span></span></a>安全防护</h3><p>安全防护功能让你能够定义特定的结构和类型，并利用这些定义来校验和修正大语言模型（LLM）生成的输出。以下是来自 <a href=https://github.com/guardrails-ai/guardrails><code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">guardrails-ai/guardrails</code> README</a> 的一个入门示例：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>from</span><span> pydantic </span><span>import</span><span> BaseModel</span><span>,</span><span> Field</span></div><div><span></span><span>from</span><span> guardrails </span><span>import</span><span> Guard</span></div><div><span></span><span>import</span><span> openai</span></div><div><span>
</span></div><div><span></span><span>class</span><span> </span><span>Pet</span><span>(</span><span>BaseModel</span><span>)</span><span>:</span><span></span></div><div><span>    pet_type</span><span>:</span><span> </span><span>str</span><span> </span><span>=</span><span> Field</span><span>(</span><span>description</span><span>=</span><span>&#34;Species of pet&#34;</span><span>)</span><span></span></div><div><span>    name</span><span>:</span><span> </span><span>str</span><span> </span><span>=</span><span> Field</span><span>(</span><span>description</span><span>=</span><span>&#34;a unique pet name&#34;</span><span>)</span><span></span></div><div><span>
</span></div><div><span>prompt </span><span>=</span><span> </span><span>&#34;&#34;&#34;</span></div><div><span>    What kind of pet should I get and what should I name it?</span></div><div><span>
</span></div><div><span>    ${gr.complete_json_suffix_v2}</span></div><div><span>&#34;&#34;&#34;</span><span></span></div><div><span>guard </span><span>=</span><span> Guard</span><span>.</span><span>from_pydantic</span><span>(</span><span>output_class</span><span>=</span><span>Pet</span><span>,</span><span> prompt</span><span>=</span><span>prompt</span><span>)</span><span></span></div><div><span>
</span></div><div><span>validated_output</span><span>,</span><span> </span><span>*</span><span>rest </span><span>=</span><span> guard</span><span>(</span><span></span></div><div><span>    llm_api</span><span>=</span><span>openai</span><span>.</span><span>completions</span><span>.</span><span>create</span><span>,</span><span></span></div><div><span>    engine</span><span>=</span><span>&#34;gpt-3.5-turbo-instruct&#34;</span><span></span></div><div><span></span><span>)</span><span></span></div><div><span>
</span></div><div><span></span><span>print</span><span>(</span><span>f&#34;</span><span>{</span><span>validated_output</span><span>}</span><span>&#34;</span><span>)</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>{</span></div><div><span>    &#34;pet_type&#34;: &#34;dog&#34;,</span></div><div><span>    &#34;name&#34;: &#34;Buddy</span></div></pre></pre><p>这个过程是怎样进行的？结构化输出和验证又是如何实现的呢？通过查看 mitmproxy 的用户界面，我发现上面的代码触发了两次大语言模型 API 的调用，第一次调用用到了这样一个提示：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>What kind of pet should I get and what should I name it?</span></div><div><span>
</span></div><div><span>
</span></div><div><span>Given below is XML that describes the information to extract from this document and the tags to extract it into.</span></div><div><span>
</span></div><div><span>&lt;output&gt;</span></div><div><span>    &lt;string name=&#34;pet_type&#34; description=&#34;Species of pet&#34;/&gt;</span></div><div><span>    &lt;string name=&#34;name&#34; description=&#34;a unique pet name&#34;/&gt;</span></div><div><span>&lt;/output&gt;</span></div><div><span>
</span></div><div><span>
</span></div><div><span>ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML&#39;s tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.</span></div><div><span>
</span></div><div><span>Here are examples of simple (XML, JSON) pairs that show the expected behavior:</span></div><div><span>- `&lt;string name=&#39;foo&#39; format=&#39;two-words lower-case&#39; /&gt;` =&gt; `{&#39;foo&#39;: &#39;example one&#39;}`</span></div><div><span>- `&lt;list name=&#39;bar&#39;&gt;&lt;string format=&#39;upper-case&#39; /&gt;&lt;/list&gt;` =&gt; `{&#34;bar&#34;: [&#39;STRING ONE&#39;, &#39;STRING TWO&#39;, etc.]}`</span></div><div><span>- `&lt;object name=&#39;baz&#39;&gt;&lt;string name=&#34;foo&#34; format=&#34;capitalize two-words&#34; /&gt;&lt;integer name=&#34;index&#34; format=&#34;1-indexed&#34; /&gt;&lt;/object&gt;` =&gt; `{&#39;baz&#39;: {&#39;foo&#39;: &#39;Some String&#39;, &#39;index&#39;: 1}}`</span></div></pre></pre><p><strong>紧接着，进行了第二次调用，使用了这样一个提示：</strong><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>I was given the following response, which was not parseable as JSON.</span></div><div><span>
</span></div><div><span>&#34;{\n    \&#34;pet_type\&#34;: \&#34;dog\&#34;,\n    \&#34;name\&#34;: \&#34;Buddy&#34;</span></div><div><span>
</span></div><div><span>Help me correct this by making it valid JSON.</span></div><div><span>
</span></div><div><span>Given below is XML that describes the information to extract from this document and the tags to extract it into.</span></div><div><span>
</span></div><div><span>&lt;output&gt;</span></div><div><span>    &lt;string name=&#34;pet_type&#34; description=&#34;Species of pet&#34;/&gt;</span></div><div><span>    &lt;string name=&#34;name&#34; description=&#34;a unique pet name&#34;/&gt;</span></div><div><span>&lt;/output&gt;</span></div><div><span>
</span></div><div><span>
</span></div><div><span>ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML&#39;s tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `null`.</span></div></pre></pre><p>确实，要实现结构化输出，过程看起来颇为繁复！我们从中了解到，这个库通过使用 XML 架构来处理结构化输出，而其他方法可能采用函数调用。如果你能在揭示这一“魔法”之后，想出一个更简单或更有效的方案，那将是值得考虑的。无论如何，我们现在对其工作机制有了更深的理解，而且避免了让你陷入不必要的复杂性中，这本身就是一种进步。<h3><a href=#%E6%8C%87%E5%8D%97><span></span></a>指南</h3><p>指南功能为编写提示语提供了限制性的生成选项和编程结构。让我们通过他们教程中的一个聊天示例深入探讨一番：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>import</span><span> guidance</span></div><div><span>gpt35 </span><span>=</span><span> guidance</span><span>.</span><span>models</span><span>.</span><span>OpenAI</span><span>(</span><span>&#34;gpt-3.5-turbo&#34;</span><span>)</span><span></span></div><div><span>
</span></div><div><span></span><span>import</span><span> re</span></div><div><span></span><span>from</span><span> guidance </span><span>import</span><span> gen</span><span>,</span><span> select</span><span>,</span><span> system</span><span>,</span><span> user</span><span>,</span><span> assistant</span></div><div><span>
</span></div><div><span></span><span>@guidance</span><span></span></div><div><span></span><span>def</span><span> </span><span>plan_for_goal</span><span>(</span><span>lm</span><span>,</span><span> goal</span><span>:</span><span> </span><span>str</span><span>)</span><span>:</span><span></span></div><div><span>
</span></div><div><span>    </span><span># This is a helper function which we will use below</span><span></span></div><div><span>    </span><span>def</span><span> </span><span>parse_best</span><span>(</span><span>prosandcons</span><span>,</span><span> options</span><span>)</span><span>:</span><span></span></div><div><span>        best </span><span>=</span><span> re</span><span>.</span><span>search</span><span>(</span><span>r&#39;Best=(\d+)&#39;</span><span>,</span><span> prosandcons</span><span>)</span><span></span></div><div><span>        </span><span>if</span><span> </span><span>not</span><span> best</span><span>:</span><span></span></div><div><span>            best </span><span>=</span><span>  re</span><span>.</span><span>search</span><span>(</span><span>r&#39;Best.*?(\d+)&#39;</span><span>,</span><span> </span><span>&#39;Best= option is 3&#39;</span><span>)</span><span></span></div><div><span>        </span><span>if</span><span> best</span><span>:</span><span></span></div><div><span>            best </span><span>=</span><span> </span><span>int</span><span>(</span><span>best</span><span>.</span><span>group</span><span>(</span><span>1</span><span>)</span><span>)</span><span></span></div><div><span>        </span><span>else</span><span>:</span><span></span></div><div><span>            best </span><span>=</span><span> </span><span>0</span><span></span></div><div><span>        </span><span>return</span><span> options</span><span>[</span><span>best</span><span>]</span><span></span></div><div><span>
</span></div><div><span>    </span><span># Some general instruction to the model</span><span></span></div><div><span>    </span><span>with</span><span> system</span><span>(</span><span>)</span><span>:</span><span></span></div><div><span>        lm </span><span>+=</span><span> </span><span>&#34;You are a helpful assistant.&#34;</span><span></span></div><div><span>
</span></div><div><span>    </span><span># Simulate a simple request from the user</span><span></span></div><div><span>    </span><span># Note that we switch to using &#39;lm2&#39; here, because these are intermediate steps (so we don&#39;t want to overwrite the current lm object)</span><span></span></div><div><span>    </span><span>with</span><span> user</span><span>(</span><span>)</span><span>:</span><span></span></div><div><span>        lm2 </span><span>=</span><span> lm </span><span>+</span><span> </span><span>f&#34;&#34;&#34;\</span></div><div><span>        I want to </span><span>{</span><span>goal</span><span>}</span><span></span></div><div><span>        Can you please generate one option for how to accomplish this?</span></div><div><span>        Please make the option very short, at most one line.&#34;&#34;&#34;</span><span></span></div><div><span>
</span></div><div><span>    </span><span># Generate several options. Note that this means several sequential generation requests</span><span></span></div><div><span>    n_options </span><span>=</span><span> </span><span>5</span><span></span></div><div><span>    </span><span>with</span><span> assistant</span><span>(</span><span>)</span><span>:</span><span></span></div><div><span>        options </span><span>=</span><span> </span><span>[</span><span>]</span><span></span></div><div><span>        </span><span>for</span><span> i </span><span>in</span><span> </span><span>range</span><span>(</span><span>n_options</span><span>)</span><span>:</span><span></span></div><div><span>            options</span><span>.</span><span>append</span><span>(</span><span>(</span><span>lm2 </span><span>+</span><span> gen</span><span>(</span><span>name</span><span>=</span><span>&#39;option&#39;</span><span>,</span><span> temperature</span><span>=</span><span>1.0</span><span>,</span><span> max_tokens</span><span>=</span><span>50</span><span>)</span><span>)</span><span>[</span><span>&#34;option&#34;</span><span>]</span><span>)</span><span></span></div><div><span>
</span></div><div><span>    </span><span># Have the user request pros and cons</span><span></span></div><div><span>    </span><span>with</span><span> user</span><span>(</span><span>)</span><span>:</span><span></span></div><div><span>        lm2 </span><span>+=</span><span> </span><span>f&#34;&#34;&#34;\</span></div><div><span>        I want to </span><span>{</span><span>goal</span><span>}</span><span></span></div><div><span>        Can you please comment on the pros and cons of each of the following options, and then pick the best option?</span></div><div><span>        ---</span></div><div><span>        &#34;&#34;&#34;</span><span></span></div><div><span>        </span><span>for</span><span> i</span><span>,</span><span> opt </span><span>in</span><span> </span><span>enumerate</span><span>(</span><span>options</span><span>)</span><span>:</span><span></span></div><div><span>            lm2 </span><span>+=</span><span> </span><span>f&#34;Option </span><span>{</span><span>i</span><span>}</span><span>: </span><span>{</span><span>opt</span><span>}</span><span>\n&#34;</span><span></span></div><div><span>        lm2 </span><span>+=</span><span> </span><span>f&#34;&#34;&#34;\</span></div><div><span>        ---</span></div><div><span>        Please discuss each option very briefly (one line for pros, one for cons), and end by saying Best=X, where X is the number of the best option.&#34;&#34;&#34;</span><span></span></div><div><span>
</span></div><div><span>    </span><span># Get the pros and cons from the model</span><span></span></div><div><span>    </span><span>with</span><span> assistant</span><span>(</span><span>)</span><span>:</span><span></span></div><div><span>        lm2 </span><span>+=</span><span> gen</span><span>(</span><span>name</span><span>=</span><span>&#39;prosandcons&#39;</span><span>,</span><span> temperature</span><span>=</span><span>0.0</span><span>,</span><span> max_tokens</span><span>=</span><span>600</span><span>,</span><span> stop</span><span>=</span><span>&#34;Best=&#34;</span><span>)</span><span> </span><span>+</span><span> </span><span>&#34;Best=&#34;</span><span> </span><span>+</span><span> gen</span><span>(</span><span>&#34;best&#34;</span><span>,</span><span> regex</span><span>=</span><span>&#34;[0-9]+&#34;</span><span>)</span><span></span></div><div><span>
</span></div><div><span>    </span><span># The user now extracts the one selected as the best, and asks for a full plan</span><span></span></div><div><span>    </span><span># We switch back to &#39;lm&#39; because this is the final result we want</span><span></span></div><div><span>    </span><span>with</span><span> user</span><span>(</span><span>)</span><span>:</span><span></span></div><div><span>        lm </span><span>+=</span><span> </span><span>f&#34;&#34;&#34;\</span></div><div><span>        I want to </span><span>{</span><span>goal</span><span>}</span><span></span></div><div><span>        Here is my plan: </span><span>{</span><span>options</span><span>[</span><span>int</span><span>(</span><span>lm2</span><span>[</span><span>&#34;best&#34;</span><span>]</span><span>)</span><span>]</span><span>}</span><span></span></div><div><span>        Please elaborate on this plan, and tell me how to best accomplish it.&#34;&#34;&#34;</span><span></span></div><div><span>
</span></div><div><span>    </span><span># The plan is generated</span><span></span></div><div><span>    </span><span>with</span><span> assistant</span><span>(</span><span>)</span><span>:</span><span></span></div><div><span>        lm </span><span>+=</span><span> gen</span><span>(</span><span>name</span><span>=</span><span>&#39;plan&#39;</span><span>,</span><span> max_tokens</span><span>=</span><span>500</span><span>)</span><span></span></div><div><span>
</span></div><div><span>    </span><span>return</span><span> lm</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>results </span><span>=</span><span> gpt35 </span><span>+</span><span> plan_for_goal</span><span>(</span><span>goal</span><span>=</span><span>&#34;read more books&#34;</span><span>)</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-"><div><span>systemYou are a helpful assistant.userI want to read more books</span></div><div><span>Here is my plan: Set aside 30 minutes of dedicated reading time each day.</span></div><div><span>Please elaborate on this plan, and tell me how to best accomplish it.assistantSetting aside 30 minutes of dedicated reading time each day is a great plan to read more books. Here are some tips to help you accomplish this goal:</span></div><div><span>
</span></div><div><span>1. Establish a routine: Choose a specific time of day that works best for you, whether it&#39;s in the morning, during lunch break, or before bed. Consistency is key to forming a habit.</span></div><div><span>
</span></div><div><span>2. Create a reading-friendly environment: Find a quiet and comfortable spot where you can focus on your reading without distractions. It could be a cozy corner in your home, a park bench, or a local library.</span></div><div><span>
</span></div><div><span>3. Minimize distractions: Put away your phone, turn off the TV, and avoid any other potential interruptions during your dedicated reading time. This will help you stay focused and fully immerse yourself in the book.</span></div><div><span>
</span></div><div><span>4. Choose books that interest you: Select books that align with your personal interests, hobbies, or goals. When you&#39;re genuinely interested in the subject matter, you&#39;ll be more motivated to read regularly.</span></div><div><span>
</span></div><div><span>5. Start with manageable goals: If you&#39;re new to reading or have a busy schedule, start with a smaller time commitment, such as 15 minutes, and gradually increase it to 30 minutes or more as you become more comfortable.</span></div><div><span>
</span></div><div><span>6. Set a timer: Use a timer or a reading app that allows you to track your reading time. This will help you stay accountable and ensure that you dedicate the full 30 minutes to reading.</span></div><div><span>
</span></div><div><span>7. Make reading enjoyable: Create a cozy reading atmosphere by lighting a candle, sipping a cup of tea, or playing soft background music. Engaging all your senses can enhance your reading experience.</span></div><div><span>
</span></div><div><span>8. Join a book club or reading group: Consider joining a book club or participating in a reading group to connect with fellow book lovers. This can provide additional motivation, discussion opportunities, and book recommendations.</span></div><div><span>
</span></div><div><span>9. Keep a reading log: Maintain a record of the books you&#39;ve read, along with your thoughts and reflections. This can help you track your progress, discover patterns in your reading preferences, and serve as a source of inspiration for future reading.</span></div><div><span>
</span></div><div><span>10. Be flexible: While it&#39;s important to have a dedicated reading time, be flexible and adaptable. Life can sometimes get busy, so if you miss a day, don&#39;t be discouraged. Simply pick up where you left off and continue with your reading routine.</span></div><div><span>
</span></div><div><span>Remember, the goal is to enjoy the process of reading and make it a regular part of your life. Happy reading!</span></div></pre></pre><p>看上去真的很不错！但具体是怎么回事呢？这实际上包括了对 OpenAI 的 7 次调用，我把它们整理在了<a href=https://gist.github.com/hamelsmu/d0d75bf702e56987f35cb715f7da4d6a>这个 gist 里</a>。这其中有 5 次是大语言模型进行“内部思维”，以产生创意的 API 调用。尽管温度被设置为 1.0，但这些“创意”大部分是重复的。OpenAI 的倒数第二次调用整理出了这些“创意”，如下所示：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>I want to read more books</span></div><div><span>Can you please comment on the pros and cons of each of the following options, and then pick the best option?</span></div><div><span>---</span></div><div><span>Option 0: Set aside dedicated time each day for reading.</span></div><div><span>Option 1: Set aside 30 minutes of dedicated reading time each day.</span></div><div><span>Option 2: Set aside dedicated time each day for reading.</span></div><div><span>Option 3: Set aside dedicated time each day for reading.</span></div><div><span>Option 4: Join a book club.</span></div><div><span>---</span></div><div><span>Please discuss each option very briefly (one line for pros, one for cons), and end by saying Best=X, where X is the number of the best option.</span></div></pre></pre><p>根据我的经验，如果你让语言模型一次性生成创意，效果可能会更好。这样，大语言模型能够参照之前的创意，从而达到更多的多样性。这是一个关于无意中增加复杂性的典型例子：盲目地应用这种设计模式很容易。虽然这不是对这一特定框架的批判——代码已经明确说明了会进行 5 次独立的调用。不过，检查你的工作通过审查 API 调用总是一个好主意。<h3><a href=#langchain><span></span></a>Langchain</h3><p>Langchain 是一个针对所有大语言模型相关任务的多功能工具。许多人在开始使用大语言模型时都会选择依赖 Langchain。鉴于 Langchain 涵盖了广泛的功能，我在这里选取两个示例进行说明。<h4><a href=#lcel-%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86%E8%A7%A3%E6%9E%90><span></span></a>LCEL 批量处理解析</h4><p>首先，我们一起探索他们最新的 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">LCEL</code>（langchain expression language）指南提供的<a href=https://python.langchain.com/docs/expression_language/why#batch>这个例子</a>：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>from</span><span> langchain_openai </span><span>import</span><span> ChatOpenAI</span></div><div><span></span><span>from</span><span> langchain_core</span><span>.</span><span>prompts </span><span>import</span><span> ChatPromptTemplate</span></div><div><span></span><span>from</span><span> langchain_core</span><span>.</span><span>output_parsers </span><span>import</span><span> StrOutputParser</span></div><div><span></span><span>from</span><span> langchain_core</span><span>.</span><span>runnables </span><span>import</span><span> RunnablePassthrough</span></div><div><span>
</span></div><div><span>prompt </span><span>=</span><span> ChatPromptTemplate</span><span>.</span><span>from_template</span><span>(</span><span></span></div><div><span>    </span><span>&#34;Tell me a short joke about {topic}&#34;</span><span></span></div><div><span></span><span>)</span><span></span></div><div><span>output_parser </span><span>=</span><span> StrOutputParser</span><span>(</span><span>)</span><span></span></div><div><span>model </span><span>=</span><span> ChatOpenAI</span><span>(</span><span>model</span><span>=</span><span>&#34;gpt-3.5-turbo&#34;</span><span>)</span><span></span></div><div><span>chain </span><span>=</span><span> </span><span>(</span><span></span></div><div><span>    </span><span>{</span><span>&#34;topic&#34;</span><span>:</span><span> RunnablePassthrough</span><span>(</span><span>)</span><span>}</span><span></span></div><div><span>    </span><span>|</span><span> prompt</span></div><div><span>    </span><span>|</span><span> model</span></div><div><span>    </span><span>|</span><span> output_parser</span></div><div><span></span><span>)</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>chain</span><span>.</span><span>batch</span><span>(</span><span>[</span><span>&#34;ice cream&#34;</span><span>,</span><span> </span><span>&#34;spaghetti&#34;</span><span>,</span><span> </span><span>&#34;dumplings&#34;</span><span>,</span><span> </span><span>&#34;tofu&#34;</span><span>,</span><span> </span><span>&#34;pizza&#34;</span><span>]</span><span>)</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>[&#34;Why did the ice cream go to therapy?\n\nBecause it had too many toppings and couldn&#39;t find its flavor!&#34;,</span></div><div><span> &#39;Why did the tomato turn red?\n\nBecause it saw the spaghetti sauce!&#39;,</span></div><div><span> &#39;Why did the dumpling go to the bakery?\n\nBecause it kneaded some company!&#39;,</span></div><div><span> &#39;Why did the tofu go to the party?\n\nBecause it wanted to blend in with the crowd!&#39;,</span></div><div><span> &#39;Why did the pizza go to the wedding?\n\nBecause it wanted to be a little cheesy!&#39;]</span></div></pre></pre><p>颇为引人注目，不是吗？这究竟是怎么回事呢？当我用 mitmproxy 工具深入观察时，发现了 <em>五个独立的</em> API 调用：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>{ &#34;messages&#34;: [{&#34;content&#34;: &#34;Tell me a short joke about spaghetti&#34;, &#34;role&#34;: &#34;user&#34;}],</span></div><div><span>  &#34;model&#34;: &#34;gpt-3.5-turbo&#34;, &#34;n&#34;: 1, &#34;stream&#34;: false, &#34;temperature&#34;: 0.7}</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>{ &#34;messages&#34;: [{&#34;content&#34;: &#34;Tell me a short joke about ice cream&#34;, &#34;role&#34;: &#34;user&#34;}],</span></div><div><span>  &#34;model&#34;: &#34;gpt-3.5-turbo&#34;, &#34;n&#34;: 1, &#34;stream&#34;: false, &#34;temperature&#34;: 0.7}</span></div></pre></pre><p>...针对列表中的每一项都进行了类似的调用。<p>向 OpenAI 发起五个独立的请求（虽然是异步的），可能并不符合你的期望，因为 <a href=https://platform.openai.com/docs/guides/rate-limits/batching-requests>OpenAI API 支持批量请求</a>。<a href=https://hamel.dev/blog/posts/prompt/#fn1>1</a> 我自己在使用 LCEL 时也遇到了速率限制问题——直到我仔细检查了 API 调用后，我才真正理解了问题所在！（很容易被“批量处理”这个词误导）。<h4><a href=#%E6%99%BA%E8%83%BD%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%93%BE><span></span></a>智能大语言模型链</h4><p>下一步，我们将探讨一个自动化工具——特别指的是 <a href=https://api.python.langchain.com/en/latest/smart_llm/langchain_experimental.smart_llm.base.SmartLLMChain.html>SmartLLMChain</a>，它能够自动为你生成编写提示：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>from</span><span> langchain</span><span>.</span><span>prompts </span><span>import</span><span> PromptTemplate</span></div><div><span></span><span>from</span><span> langchain_experimental</span><span>.</span><span>smart_llm </span><span>import</span><span> SmartLLMChain</span></div><div><span></span><span>from</span><span> langchain_openai </span><span>import</span><span> ChatOpenAI</span></div><div><span>
</span></div><div><span>hard_question </span><span>=</span><span> &#34;I have a </span><span>12</span><span> liter jug </span><span>and</span><span> a </span><span>6</span><span> liter jug</span><span>.</span><span>\</span></div><div><span>I want to measure </span><span>6</span><span> liters</span><span>.</span><span> How do I do it?&#34;</span></div><div><span>prompt </span><span>=</span><span> PromptTemplate</span><span>.</span><span>from_template</span><span>(</span><span>hard_question</span><span>)</span><span></span></div><div><span>llm </span><span>=</span><span> ChatOpenAI</span><span>(</span><span>temperature</span><span>=</span><span>0</span><span>,</span><span> model_name</span><span>=</span><span>&#34;gpt-3.5-turbo&#34;</span><span>)</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>chain </span><span>=</span><span> SmartLLMChain</span><span>(</span><span>llm</span><span>=</span><span>llm</span><span>,</span><span> prompt</span><span>=</span><span>prompt</span><span>,</span><span></span></div><div><span>                      n_ideas</span><span>=</span><span>2</span><span>,</span><span></span></div><div><span>                      verbose</span><span>=</span><span>True</span><span>)</span><span></span></div><div><span>result </span><span>=</span><span> chain</span><span>.</span><span>run</span><span>(</span><span>{</span><span>}</span><span>)</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>print</span><span>(</span><span>result</span><span>)</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>Idea 1: 1. Fill the 12 liter jug completely.</span></div><div><span>2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.</span></div><div><span>3. Empty the 6 liter jug.</span></div><div><span>4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.</span></div><div><span>5. You now have 6 liters in the 6 liter jug.</span></div><div><span>
</span></div><div><span>Idea 2: 1. Fill the 12 liter jug completely.</span></div><div><span>2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.</span></div><div><span>3. Empty the 6 liter jug.</span></div><div><span>4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.</span></div><div><span>5. You now have 6 liters in the 6 liter jug.</span></div><div><span>
</span></div><div><span>Improved Answer:</span></div><div><span>1. Fill the 12 liter jug completely.</span></div><div><span>2. Pour the contents of the 12 liter jug into the 6 liter jug until the 6 liter jug is full. This will leave you with 6 liters in the 12 liter jug and the 6 liter jug completely filled.</span></div><div><span>3. Empty the 6 liter jug.</span></div><div><span>4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.</span></div><div><span>5. You now have 6 liters in the 6 liter jug.</span></div><div><span>
</span></div><div><span>Full Answer:</span></div><div><span>To measure 6 liters using a 12 liter jug and a 6 liter jug, follow these steps:</span></div><div><span>1. Fill the 12 liter jug completely.</span></div><div><span>2. Pour the contents of the 12 liter jug into the 6 liter jug until the 6 liter jug is full. This will leave you with 6 liters in the 12 liter jug and the 6 liter jug completely filled.</span></div><div><span>3. Empty the 6 liter jug.</span></div><div><span>4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.</span></div><div><span>5. You now have 6 liters in the 6 liter jug.</span></div></pre></pre><p>有趣的是，这个工具的运作方式如何呢？虽然该 API 提供的日志详细记录了许多信息（详见 <a href=https://gist.github.com/hamelsmu/abfb14b0af4c70e8532f9d4e0ef3e54e>这个链接</a>），但其请求模式特别值得关注：<ol><li><p>每个“创意”需要两次独立的 API 调用。<li><p>另一次 API 调用则结合了两个创意作为背景，附带以下提示：<blockquote><p>作为一名研究员，你的任务是调查提供的两个响应选项。列举每个选项的缺点和逻辑漏洞。我们将一步步检查，确保无遗漏：”</blockquote><li><p>最后，根据第二步的批判性反馈，进行另一次 API 调用以生成答案。</ol><p>目前还不清楚这种做法是否为最佳。我对是否需要四次独立的 API 调用来完成此任务表示怀疑。或许可以将批评和最终答案的生成合并为一步？此外，提示中存在拼写错误（<code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">Let&#39;w</code>），并且过分强调识别问题，这让人怀疑这个提示是否经过优化或测试。<h3><a href=#%E6%95%99%E7%BB%83><span></span></a>教练</h3><p><a href=https://github.com/jxnl/instructor>教练</a> 是一个专为结构化输出设计的框架。<h4><a href=#%E5%88%A9%E7%94%A8-pydantic-%E8%BF%9B%E8%A1%8C%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96><span></span></a>利用 Pydantic 进行结构化数据提取<a href=https://hamel.dev/blog/posts/prompt/#structred-data-extraction-with-pydantic></a></h4><p>以下是一个利用 Pydantic 定义模式以提取结构化数据的基础示例，源自项目的 <a href=https://github.com/jxnl/instructor>README</a>。<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>import</span><span> instructor</span></div><div><span></span><span>from</span><span> openai </span><span>import</span><span> OpenAI</span></div><div><span></span><span>from</span><span> pydantic </span><span>import</span><span> BaseModel</span></div><div><span>
</span></div><div><span>client </span><span>=</span><span> instructor</span><span>.</span><span>patch</span><span>(</span><span>OpenAI</span><span>(</span><span>)</span><span>)</span><span></span></div><div><span>
</span></div><div><span></span><span>class</span><span> </span><span>UserDetail</span><span>(</span><span>BaseModel</span><span>)</span><span>:</span><span></span></div><div><span>    name</span><span>:</span><span> </span><span>str</span><span></span></div><div><span>    age</span><span>:</span><span> </span><span>int</span><span></span></div><div><span>
</span></div><div><span>user </span><span>=</span><span> client</span><span>.</span><span>chat</span><span>.</span><span>completions</span><span>.</span><span>create</span><span>(</span><span></span></div><div><span>    model</span><span>=</span><span>&#34;gpt-3.5-turbo&#34;</span><span>,</span><span></span></div><div><span>    response_model</span><span>=</span><span>UserDetail</span><span>,</span><span></span></div><div><span>    messages</span><span>=</span><span>[</span><span>{</span><span>&#34;role&#34;</span><span>:</span><span> </span><span>&#34;user&#34;</span><span>,</span><span> </span><span>&#34;content&#34;</span><span>:</span><span> </span><span>&#34;Extract Jason is 25 years old&#34;</span><span>}</span><span>]</span><span>)</span></div></pre></pre><p>通过分析 mitmproxy 记录的 API 调用，我们可以清晰地看到其工作原理：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>{</span></div><div><span>    &#34;function_call&#34;: {</span></div><div><span>        &#34;name&#34;: &#34;UserDetail&#34;</span></div><div><span>    },</span></div><div><span>    &#34;functions&#34;: [</span></div><div><span>        {</span></div><div><span>            &#34;description&#34;: &#34;Correctly extracted `UserDetail` with all the required parameters with correct types&#34;,</span></div><div><span>            &#34;name&#34;: &#34;UserDetail&#34;,</span></div><div><span>            &#34;parameters&#34;: {</span></div><div><span>                &#34;properties&#34;: {</span></div><div><span>                    &#34;age&#34;: {</span></div><div><span>                        &#34;title&#34;: &#34;Age&#34;,</span></div><div><span>                        &#34;type&#34;: &#34;integer&#34;</span></div><div><span>                    },</span></div><div><span>                    &#34;name&#34;: {</span></div><div><span>                        &#34;title&#34;: &#34;Name&#34;,</span></div><div><span>                        &#34;type&#34;: &#34;string&#34;</span></div><div><span>                    }</span></div><div><span>                },</span></div><div><span>                &#34;required&#34;: [</span></div><div><span>                    &#34;age&#34;,</span></div><div><span>                    &#34;name&#34;</span></div><div><span>                ],</span></div><div><span>                &#34;type&#34;: &#34;object&#34;</span></div><div><span>            }</span></div><div><span>        }</span></div><div><span>    ],</span></div><div><span>    &#34;messages&#34;: [</span></div><div><span>        {</span></div><div><span>            &#34;content&#34;: &#34;Extract Jason is 25 years old&#34;,</span></div><div><span>            &#34;role&#34;: &#34;user&#34;</span></div><div><span>        }</span></div><div><span>    ],</span></div><div><span>    &#34;model&#34;: &#34;gpt-3.5-turbo&#34;</span></div><div><span>}</span></div></pre></pre><p>这非常棒。对于需要结构化输出的场景——<strong>它完全满足了我的需求，并以我手动操作时相同的方式正确使用了 OpenAI API</strong>（即通过定义函数模式）。我认为这个 API 是一种高效的简化方式，它完全按照预期工作，且操作简洁。<h4><a href=#%E9%AA%8C%E8%AF%81><span></span></a>验证</h4><p>不过，有些 API 的设计更为前卫，它们甚至可以帮你直接生成提示。以这个<a href=https://jxnl.github.io/instructor/tutorials/4-validation/>验证示例</a>为例。深入探讨这个案例，你会遇到一些问题，这些问题与之前讨论的<a href=https://hamel.dev/blog/posts/prompt/#SmartLLMChain>Langchain 的 SmartLLMChain</a>相似。在此示例中，你将看到进行了 3 次大语言模型 API 调用才得出正确的答案，最终提交的数据内容如下所示：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-auto"><div><span>{</span></div><div><span>    &#34;function_call&#34;: {</span></div><div><span>        &#34;name&#34;: &#34;Validator&#34;</span></div><div><span>    },</span></div><div><span>    &#34;functions&#34;: [</span></div><div><span>        {</span></div><div><span>            &#34;description&#34;: &#34;Validate if an attribute is correct and if not,\nreturn a new value with an error message&#34;,</span></div><div><span>            &#34;name&#34;: &#34;Validator&#34;,</span></div><div><span>            &#34;parameters&#34;: {</span></div><div><span>                &#34;properties&#34;: {</span></div><div><span>                    &#34;fixed_value&#34;: {</span></div><div><span>                        &#34;anyOf&#34;: [</span></div><div><span>                            {</span></div><div><span>                                &#34;type&#34;: &#34;string&#34;</span></div><div><span>                            },</span></div><div><span>                            {</span></div><div><span>                                &#34;type&#34;: &#34;null&#34;</span></div><div><span>                            }</span></div><div><span>                        ],</span></div><div><span>                        &#34;default&#34;: null,</span></div><div><span>                        &#34;description&#34;: &#34;If the attribute is not valid, suggest a new value for the attribute&#34;,</span></div><div><span>                        &#34;title&#34;: &#34;Fixed Value&#34;</span></div><div><span>                    },</span></div><div><span>                    &#34;is_valid&#34;: {</span></div><div><span>                        &#34;default&#34;: true,</span></div><div><span>                        &#34;description&#34;: &#34;Whether the attribute is valid based on the requirements&#34;,</span></div><div><span>                        &#34;title&#34;: &#34;Is Valid&#34;,</span></div><div><span>                        &#34;type&#34;: &#34;boolean&#34;</span></div><div><span>                    },</span></div><div><span>                    &#34;reason&#34;: {</span></div><div><span>                        &#34;anyOf&#34;: [</span></div><div><span>                            {</span></div><div><span>                                &#34;type&#34;: &#34;string&#34;</span></div><div><span>                            },</span></div><div><span>                            {</span></div><div><span>                                &#34;type&#34;: &#34;null&#34;</span></div><div><span>                            }</span></div><div><span>                        ],</span></div><div><span>                        &#34;default&#34;: null,</span></div><div><span>                        &#34;description&#34;: &#34;The error message if the attribute is not valid, otherwise None&#34;,</span></div><div><span>                        &#34;title&#34;: &#34;Reason&#34;</span></div><div><span>                    }</span></div><div><span>                },</span></div><div><span>                &#34;required&#34;: [],</span></div><div><span>                &#34;type&#34;: &#34;object&#34;</span></div><div><span>            }</span></div><div><span>        }</span></div><div><span>    ],</span></div><div><span>    &#34;messages&#34;: [</span></div><div><span>        {</span></div><div><span>            &#34;content&#34;: &#34;You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.&#34;,</span></div><div><span>            &#34;role&#34;: &#34;system&#34;</span></div><div><span>        },</span></div><div><span>        {</span></div><div><span>            &#34;content&#34;: &#34;Does `According to some perspectives, the meaning of life is to find purpose, happiness, and fulfillment. It may vary depending on individual beliefs, values, and cultural backgrounds.` follow the rules: don&#39;t say objectionable things&#34;,</span></div><div><span>            &#34;role&#34;: &#34;user&#34;</span></div><div><span>        }</span></div><div><span>    ],</span></div><div><span>    &#34;model&#34;: &#34;gpt-3.5-turbo&#34;,</span></div><div><span>    &#34;temperature&#34;: 0</span></div><div><span>}</span></div></pre></pre><p>我在思考，是否有可能将验证和修正步骤合二为一，仅通过一次大语言模型调用完成。另外，我也在考虑，使用通用验证函数（正如上述数据内容中所提供的）来评估输出结果是否是一种恰当的方法？虽然我还没有确切的答案，但认为这种设计模式非常有探索价值。<p>注意事项<p>谈到大语言模型的框架，我特别推崇这一套。通过 Pydantic 定义数据模式的核心功能极其便利，代码的可读性和易理解性也非常好。即便如此，我还是发现，通过拦截 API 调用来获得不同的视角是非常有益的。<p>尽管可以在该系统中设定日志级别来查看原始的 API 调用数据，但我更倾向于使用一个不依赖于特定框架的方法来进行 :)<h3><a href=#dspy><span></span></a>DSPy</h3><p><a href=https://github.com/stanfordnlp/dspy>DSPy</a> 是一个框架，它能帮助你通过优化提示来改善任何指定指标。学习 DSPy 的过程相对有些挑战，这部分是因为它引入了很多新的、特有的技术术语，比如编译器和提词器。但是，一旦我们深入了解它所进行的 API 调用，这层复杂性就能迅速被揭开了！<p>来看看<a href=https://dspy-docs.vercel.app/docs/quick-start/minimal-example>最简示例</a>的运行情况：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>import</span><span> time</span></div><div><span></span><span>import</span><span> dspy</span></div><div><span></span><span>from</span><span> dspy</span><span>.</span><span>datasets</span><span>.</span><span>gsm8k </span><span>import</span><span> GSM8K</span><span>,</span><span> gsm8k_metric</span></div><div><span>start_time </span><span>=</span><span> time</span><span>.</span><span>time</span><span>(</span><span>)</span><span></span></div><div><span>
</span></div><div><span></span><span># Set up the LM</span><span></span></div><div><span>turbo </span><span>=</span><span> dspy</span><span>.</span><span>OpenAI</span><span>(</span><span>model</span><span>=</span><span>&#39;gpt-3.5-turbo-instruct&#39;</span><span>,</span><span> max_tokens</span><span>=</span><span>250</span><span>)</span><span></span></div><div><span>dspy</span><span>.</span><span>settings</span><span>.</span><span>configure</span><span>(</span><span>lm</span><span>=</span><span>turbo</span><span>)</span><span></span></div><div><span>
</span></div><div><span></span><span># Load math questions from the GSM8K dataset</span><span></span></div><div><span>gms8k </span><span>=</span><span> GSM8K</span><span>(</span><span>)</span><span></span></div><div><span>trainset</span><span>,</span><span> devset </span><span>=</span><span> gms8k</span><span>.</span><span>train</span><span>,</span><span> gms8k</span><span>.</span><span>dev</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>class</span><span> </span><span>CoT</span><span>(</span><span>dspy</span><span>.</span><span>Module</span><span>)</span><span>:</span><span></span></div><div><span>    </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>)</span><span>:</span><span></span></div><div><span>        </span><span>super</span><span>(</span><span>)</span><span>.</span><span>__init__</span><span>(</span><span>)</span><span></span></div><div><span>        self</span><span>.</span><span>prog </span><span>=</span><span> dspy</span><span>.</span><span>ChainOfThought</span><span>(</span><span>&#34;question -&gt; answer&#34;</span><span>)</span><span></span></div><div><span>
</span></div><div><span>    </span><span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span><span> question</span><span>)</span><span>:</span><span></span></div><div><span>        </span><span>return</span><span> self</span><span>.</span><span>prog</span><span>(</span><span>question</span><span>=</span><span>question</span><span>)</span></div></pre></pre><pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>from</span><span> dspy</span><span>.</span><span>teleprompt </span><span>import</span><span> BootstrapFewShotWithRandomSearch</span></div><div><span>
</span></div><div><span></span><span># Set up the optimizer: we want to &#34;bootstrap&#34; (i.e., self-generate) 8-shot examples of our CoT program.</span><span></span></div><div><span></span><span># The optimizer will repeat this 10 times (plus some initial attempts) before selecting its best attempt on the devset.</span><span></span></div><div><span>config </span><span>=</span><span> </span><span>dict</span><span>(</span><span>max_bootstrapped_demos</span><span>=</span><span>8</span><span>,</span><span> max_labeled_demos</span><span>=</span><span>8</span><span>,</span><span> num_candidate_programs</span><span>=</span><span>10</span><span>,</span><span> num_threads</span><span>=</span><span>4</span><span>)</span><span></span></div><div><span>
</span></div><div><span></span><span># Optimize! Use the `gms8k_metric` here. In general, the metric is going to tell the optimizer how well it&#39;s doing.</span><span></span></div><div><span>teleprompter </span><span>=</span><span> BootstrapFewShotWithRandomSearch</span><span>(</span><span>metric</span><span>=</span><span>gsm8k_metric</span><span>,</span><span> </span><span>**</span><span>config</span><span>)</span><span></span></div><div><span>optimized_cot </span><span>=</span><span> teleprompter</span><span>.</span><span>compile</span><span>(</span><span>CoT</span><span>(</span><span>)</span><span>,</span><span> trainset</span><span>=</span><span>trainset</span><span>,</span><span> valset</span><span>=</span><span>devset</span><span>)</span></div></pre></pre><p>结果并不像“最简”那样简单<p>虽然这是官方提供的<a href=https://dspy-docs.vercel.app/docs/quick-start/minimal-example>快速入门/最简示例</a>，但这段代码的运行时间<strong>超过了 30 分钟，并且向 OpenAI 发起了数百次请求！</strong> 这不仅消耗了大量时间（和金钱），对于初次尝试这个库的人来说，也没有任何预先的提示或警告。<p>DSPy 进行了数百次 API 调用，原因是它在为一个少样本提示迭代采样，并根据验证集上的 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">gsm8k_metric</code> 选择最佳示例。通过审查 mitmproxy 记录的 API 请求，我很快就明白了这个过程。<p>DSPy 提供了一个 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">inspect_history</code> 方法，可以让你回顾最后 <code class="relative rounded bg-muted mx-2 px-2 py-1 font-mono">n</code> 次的提示及其输出结果：<pre class="mb-4 mt-6 max-h-[650px] overflow-x-auto rounded border bg-[#1e1e1e]"><pre class="px-4 py-4 prism-code language-python"><div><span>turbo</span><span>.</span><span>inspect_history</span><span>(</span><span>n</span><span>=</span><span>1</span><span>)</span></div></pre></pre><p>我确认了这些提示与 mitmproxy 中记录的最后几次 API 调用相吻合。总而言之，我可能会考虑保留这些提示，但放弃使用这个库。不过，我还是很好奇这个库将来的发展方向。<h2><a href=#%E6%88%91%E7%9A%84%E4%B8%AA%E4%BA%BA%E4%BD%93%E9%AA%8C><span></span></a>我的个人体验<a href=https://hamel.dev/blog/posts/prompt/#my-personal-experience></a></h2><p>我对大语言模型 (LLM) 的库有偏见吗？当然不。事实上，如果能在适当的场合，考虑全面地利用，本文提到的许多库实际上非常有用。但遗憾的是，我看到太多人在不完全理解其原理和用途的情况下，盲目使用这些工具。<p>作为一名独立顾问，我始终致力于帮助客户避免不必要地增加复杂度。面对大语言模型引发的热潮，很容易被诱惑去尝试更多工具。而仔细研究提示内容正是抵制这种诱惑的有效方法之一。<p>我对那些使用户与大语言模型之间距离过远的框架持保留态度。通过勇敢地说出“别瞎忙了，直接给我看提示！”来使用这些工具，你实际上是在为自己的选择赋予了更多的主动权。<a href=https://hamel.dev/blog/posts/prompt/#fn2>2</a><p><em>感谢：非常感谢 Jeremy Howard 和 Ben Clavie 对本文的审阅和提供的深刻见解。</em><h2><a href=#%E8%84%9A%E6%B3%A8><span></span></a>脚注<a href=https://hamel.dev/blog/posts/prompt/#footnotes-1></a></h2><ol><li><p>他们采用了异步调用技术，因此不会减慢你的处理速度。<a href=https://hamel.dev/blog/posts/prompt/#fnref1>↩︎</a><li><p>无需低声说话，大胆说出来也完全没问题——让更多人听到你的声音！<a href=https://hamel.dev/blog/posts/prompt/#fnref2>↩︎</a></ol></div></main>
<hr>
<footer>
        <label>原网址: <a href="https://baoyu.io/translations/prompt-engineering/fuck-you-show-me-the-prompt" target="_blank" referrerpolicy="no-referrer" rel="noopener noreferrer">访问</a></label><br/>
        <label>创建时间: 2024-03-04 09:23:53</label><br/>
        <br/>
        
      </footer>
<link rel=stylesheet href=https://cp-bkt1.oss-cn-hangzhou.aliyuncs.com/cdn/prism.css>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css>
<script src=https://cp-bkt1.oss-cn-hangzhou.aliyuncs.com/cdn/prism.js></script>
</body>
</html>